{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9363295880149812,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0018726591760299626,
      "grad_norm": 0.686246395111084,
      "learning_rate": 0.0,
      "loss": 2.4203,
      "step": 1
    },
    {
      "epoch": 0.003745318352059925,
      "grad_norm": 0.814985454082489,
      "learning_rate": 3.1055900621118013e-07,
      "loss": 2.4452,
      "step": 2
    },
    {
      "epoch": 0.0056179775280898875,
      "grad_norm": 0.83893221616745,
      "learning_rate": 6.211180124223603e-07,
      "loss": 2.5412,
      "step": 3
    },
    {
      "epoch": 0.00749063670411985,
      "grad_norm": 0.8282657265663147,
      "learning_rate": 9.316770186335405e-07,
      "loss": 2.6115,
      "step": 4
    },
    {
      "epoch": 0.009363295880149813,
      "grad_norm": 0.7976163029670715,
      "learning_rate": 1.2422360248447205e-06,
      "loss": 2.4143,
      "step": 5
    },
    {
      "epoch": 0.011235955056179775,
      "grad_norm": 0.7247155904769897,
      "learning_rate": 1.5527950310559006e-06,
      "loss": 2.3334,
      "step": 6
    },
    {
      "epoch": 0.013108614232209739,
      "grad_norm": 0.7594788074493408,
      "learning_rate": 1.863354037267081e-06,
      "loss": 2.3139,
      "step": 7
    },
    {
      "epoch": 0.0149812734082397,
      "grad_norm": 0.7315605282783508,
      "learning_rate": 2.173913043478261e-06,
      "loss": 2.3609,
      "step": 8
    },
    {
      "epoch": 0.016853932584269662,
      "grad_norm": 0.7160460352897644,
      "learning_rate": 2.484472049689441e-06,
      "loss": 2.398,
      "step": 9
    },
    {
      "epoch": 0.018726591760299626,
      "grad_norm": 0.7922292351722717,
      "learning_rate": 2.7950310559006214e-06,
      "loss": 2.388,
      "step": 10
    },
    {
      "epoch": 0.020599250936329586,
      "grad_norm": 0.7714409828186035,
      "learning_rate": 3.1055900621118013e-06,
      "loss": 2.3865,
      "step": 11
    },
    {
      "epoch": 0.02247191011235955,
      "grad_norm": 0.6570102572441101,
      "learning_rate": 3.4161490683229816e-06,
      "loss": 2.4269,
      "step": 12
    },
    {
      "epoch": 0.024344569288389514,
      "grad_norm": 0.7235753536224365,
      "learning_rate": 3.726708074534162e-06,
      "loss": 2.4131,
      "step": 13
    },
    {
      "epoch": 0.026217228464419477,
      "grad_norm": 0.7195878624916077,
      "learning_rate": 4.037267080745342e-06,
      "loss": 2.2283,
      "step": 14
    },
    {
      "epoch": 0.028089887640449437,
      "grad_norm": 0.7368719577789307,
      "learning_rate": 4.347826086956522e-06,
      "loss": 2.2969,
      "step": 15
    },
    {
      "epoch": 0.0299625468164794,
      "grad_norm": 0.7198935151100159,
      "learning_rate": 4.658385093167702e-06,
      "loss": 2.3758,
      "step": 16
    },
    {
      "epoch": 0.031835205992509365,
      "grad_norm": 0.8465383052825928,
      "learning_rate": 4.968944099378882e-06,
      "loss": 2.5286,
      "step": 17
    },
    {
      "epoch": 0.033707865168539325,
      "grad_norm": 0.8598976135253906,
      "learning_rate": 5.279503105590062e-06,
      "loss": 2.5995,
      "step": 18
    },
    {
      "epoch": 0.035580524344569285,
      "grad_norm": 0.8012314438819885,
      "learning_rate": 5.590062111801243e-06,
      "loss": 2.3695,
      "step": 19
    },
    {
      "epoch": 0.03745318352059925,
      "grad_norm": 0.7161620259284973,
      "learning_rate": 5.900621118012423e-06,
      "loss": 2.4169,
      "step": 20
    },
    {
      "epoch": 0.03932584269662921,
      "grad_norm": 0.7271658182144165,
      "learning_rate": 6.2111801242236025e-06,
      "loss": 2.3415,
      "step": 21
    },
    {
      "epoch": 0.04119850187265917,
      "grad_norm": 0.8007901310920715,
      "learning_rate": 6.521739130434783e-06,
      "loss": 2.3747,
      "step": 22
    },
    {
      "epoch": 0.04307116104868914,
      "grad_norm": 0.7688732743263245,
      "learning_rate": 6.832298136645963e-06,
      "loss": 2.3785,
      "step": 23
    },
    {
      "epoch": 0.0449438202247191,
      "grad_norm": 0.9078900814056396,
      "learning_rate": 7.142857142857143e-06,
      "loss": 2.3047,
      "step": 24
    },
    {
      "epoch": 0.04681647940074907,
      "grad_norm": 0.7958839535713196,
      "learning_rate": 7.453416149068324e-06,
      "loss": 2.5041,
      "step": 25
    },
    {
      "epoch": 0.04868913857677903,
      "grad_norm": 0.690209150314331,
      "learning_rate": 7.763975155279503e-06,
      "loss": 2.1322,
      "step": 26
    },
    {
      "epoch": 0.05056179775280899,
      "grad_norm": 0.7943695187568665,
      "learning_rate": 8.074534161490684e-06,
      "loss": 2.3158,
      "step": 27
    },
    {
      "epoch": 0.052434456928838954,
      "grad_norm": 0.7732019424438477,
      "learning_rate": 8.385093167701864e-06,
      "loss": 2.2117,
      "step": 28
    },
    {
      "epoch": 0.054307116104868915,
      "grad_norm": 0.7079126834869385,
      "learning_rate": 8.695652173913044e-06,
      "loss": 2.2748,
      "step": 29
    },
    {
      "epoch": 0.056179775280898875,
      "grad_norm": 0.7739774584770203,
      "learning_rate": 9.006211180124225e-06,
      "loss": 2.2969,
      "step": 30
    },
    {
      "epoch": 0.05805243445692884,
      "grad_norm": 0.7427830696105957,
      "learning_rate": 9.316770186335403e-06,
      "loss": 2.4008,
      "step": 31
    },
    {
      "epoch": 0.0599250936329588,
      "grad_norm": 0.7775601148605347,
      "learning_rate": 9.627329192546584e-06,
      "loss": 2.3511,
      "step": 32
    },
    {
      "epoch": 0.06179775280898876,
      "grad_norm": 0.6674568057060242,
      "learning_rate": 9.937888198757764e-06,
      "loss": 2.3528,
      "step": 33
    },
    {
      "epoch": 0.06367041198501873,
      "grad_norm": 0.8952006101608276,
      "learning_rate": 1.0248447204968944e-05,
      "loss": 2.3574,
      "step": 34
    },
    {
      "epoch": 0.06554307116104868,
      "grad_norm": 0.6094021797180176,
      "learning_rate": 1.0559006211180125e-05,
      "loss": 2.2062,
      "step": 35
    },
    {
      "epoch": 0.06741573033707865,
      "grad_norm": 0.5548972487449646,
      "learning_rate": 1.0869565217391305e-05,
      "loss": 2.1659,
      "step": 36
    },
    {
      "epoch": 0.06928838951310862,
      "grad_norm": 0.6800743937492371,
      "learning_rate": 1.1180124223602485e-05,
      "loss": 2.2064,
      "step": 37
    },
    {
      "epoch": 0.07116104868913857,
      "grad_norm": 0.7777277827262878,
      "learning_rate": 1.1490683229813664e-05,
      "loss": 2.3027,
      "step": 38
    },
    {
      "epoch": 0.07303370786516854,
      "grad_norm": 0.7216764092445374,
      "learning_rate": 1.1801242236024846e-05,
      "loss": 2.2446,
      "step": 39
    },
    {
      "epoch": 0.0749063670411985,
      "grad_norm": 0.5404088497161865,
      "learning_rate": 1.2111801242236026e-05,
      "loss": 2.2547,
      "step": 40
    },
    {
      "epoch": 0.07677902621722846,
      "grad_norm": 0.5930318236351013,
      "learning_rate": 1.2422360248447205e-05,
      "loss": 2.231,
      "step": 41
    },
    {
      "epoch": 0.07865168539325842,
      "grad_norm": 0.49154409766197205,
      "learning_rate": 1.2732919254658385e-05,
      "loss": 2.0329,
      "step": 42
    },
    {
      "epoch": 0.08052434456928839,
      "grad_norm": 0.4442600607872009,
      "learning_rate": 1.3043478260869566e-05,
      "loss": 2.1945,
      "step": 43
    },
    {
      "epoch": 0.08239700374531835,
      "grad_norm": 0.5601586699485779,
      "learning_rate": 1.3354037267080746e-05,
      "loss": 2.2329,
      "step": 44
    },
    {
      "epoch": 0.08426966292134831,
      "grad_norm": 0.48280420899391174,
      "learning_rate": 1.3664596273291926e-05,
      "loss": 2.2162,
      "step": 45
    },
    {
      "epoch": 0.08614232209737828,
      "grad_norm": 0.5037596821784973,
      "learning_rate": 1.3975155279503105e-05,
      "loss": 2.2359,
      "step": 46
    },
    {
      "epoch": 0.08801498127340825,
      "grad_norm": 0.4836675524711609,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 2.0657,
      "step": 47
    },
    {
      "epoch": 0.0898876404494382,
      "grad_norm": 0.5286893844604492,
      "learning_rate": 1.4596273291925466e-05,
      "loss": 2.1832,
      "step": 48
    },
    {
      "epoch": 0.09176029962546817,
      "grad_norm": 0.4673212766647339,
      "learning_rate": 1.4906832298136648e-05,
      "loss": 2.1015,
      "step": 49
    },
    {
      "epoch": 0.09363295880149813,
      "grad_norm": 0.5097039341926575,
      "learning_rate": 1.5217391304347828e-05,
      "loss": 2.2279,
      "step": 50
    },
    {
      "epoch": 0.09550561797752809,
      "grad_norm": 0.48744577169418335,
      "learning_rate": 1.5527950310559007e-05,
      "loss": 2.0435,
      "step": 51
    },
    {
      "epoch": 0.09737827715355805,
      "grad_norm": 0.43019863963127136,
      "learning_rate": 1.5838509316770185e-05,
      "loss": 2.1509,
      "step": 52
    },
    {
      "epoch": 0.09925093632958802,
      "grad_norm": 0.4891473948955536,
      "learning_rate": 1.6149068322981367e-05,
      "loss": 2.2244,
      "step": 53
    },
    {
      "epoch": 0.10112359550561797,
      "grad_norm": 0.4370977282524109,
      "learning_rate": 1.645962732919255e-05,
      "loss": 2.0888,
      "step": 54
    },
    {
      "epoch": 0.10299625468164794,
      "grad_norm": 0.43379369378089905,
      "learning_rate": 1.6770186335403728e-05,
      "loss": 2.168,
      "step": 55
    },
    {
      "epoch": 0.10486891385767791,
      "grad_norm": 0.42278650403022766,
      "learning_rate": 1.7080745341614907e-05,
      "loss": 2.0247,
      "step": 56
    },
    {
      "epoch": 0.10674157303370786,
      "grad_norm": 0.392599880695343,
      "learning_rate": 1.739130434782609e-05,
      "loss": 1.9792,
      "step": 57
    },
    {
      "epoch": 0.10861423220973783,
      "grad_norm": 0.40951257944107056,
      "learning_rate": 1.7701863354037267e-05,
      "loss": 1.9536,
      "step": 58
    },
    {
      "epoch": 0.1104868913857678,
      "grad_norm": 0.46047037839889526,
      "learning_rate": 1.801242236024845e-05,
      "loss": 2.0474,
      "step": 59
    },
    {
      "epoch": 0.11235955056179775,
      "grad_norm": 0.4149409234523773,
      "learning_rate": 1.8322981366459628e-05,
      "loss": 1.9672,
      "step": 60
    },
    {
      "epoch": 0.11423220973782772,
      "grad_norm": 0.4724958837032318,
      "learning_rate": 1.8633540372670807e-05,
      "loss": 1.9774,
      "step": 61
    },
    {
      "epoch": 0.11610486891385768,
      "grad_norm": 0.508089542388916,
      "learning_rate": 1.894409937888199e-05,
      "loss": 2.0583,
      "step": 62
    },
    {
      "epoch": 0.11797752808988764,
      "grad_norm": 0.43429940938949585,
      "learning_rate": 1.9254658385093167e-05,
      "loss": 2.0286,
      "step": 63
    },
    {
      "epoch": 0.1198501872659176,
      "grad_norm": 0.3843030631542206,
      "learning_rate": 1.956521739130435e-05,
      "loss": 1.9985,
      "step": 64
    },
    {
      "epoch": 0.12172284644194757,
      "grad_norm": 0.43175414204597473,
      "learning_rate": 1.9875776397515528e-05,
      "loss": 1.8487,
      "step": 65
    },
    {
      "epoch": 0.12359550561797752,
      "grad_norm": 0.43306490778923035,
      "learning_rate": 2.0186335403726707e-05,
      "loss": 2.1376,
      "step": 66
    },
    {
      "epoch": 0.1254681647940075,
      "grad_norm": 0.453904926776886,
      "learning_rate": 2.049689440993789e-05,
      "loss": 1.8587,
      "step": 67
    },
    {
      "epoch": 0.12734082397003746,
      "grad_norm": 0.41487178206443787,
      "learning_rate": 2.080745341614907e-05,
      "loss": 1.9384,
      "step": 68
    },
    {
      "epoch": 0.12921348314606743,
      "grad_norm": 0.5272640585899353,
      "learning_rate": 2.111801242236025e-05,
      "loss": 1.9193,
      "step": 69
    },
    {
      "epoch": 0.13108614232209737,
      "grad_norm": 0.4591567814350128,
      "learning_rate": 2.1428571428571428e-05,
      "loss": 1.8715,
      "step": 70
    },
    {
      "epoch": 0.13295880149812733,
      "grad_norm": 0.4477032721042633,
      "learning_rate": 2.173913043478261e-05,
      "loss": 1.7985,
      "step": 71
    },
    {
      "epoch": 0.1348314606741573,
      "grad_norm": 0.40909329056739807,
      "learning_rate": 2.204968944099379e-05,
      "loss": 1.8429,
      "step": 72
    },
    {
      "epoch": 0.13670411985018727,
      "grad_norm": 0.4560895264148712,
      "learning_rate": 2.236024844720497e-05,
      "loss": 1.7866,
      "step": 73
    },
    {
      "epoch": 0.13857677902621723,
      "grad_norm": 0.39511895179748535,
      "learning_rate": 2.2670807453416153e-05,
      "loss": 1.8552,
      "step": 74
    },
    {
      "epoch": 0.1404494382022472,
      "grad_norm": 0.37980368733406067,
      "learning_rate": 2.2981366459627328e-05,
      "loss": 1.9842,
      "step": 75
    },
    {
      "epoch": 0.14232209737827714,
      "grad_norm": 0.3479466438293457,
      "learning_rate": 2.329192546583851e-05,
      "loss": 1.8867,
      "step": 76
    },
    {
      "epoch": 0.1441947565543071,
      "grad_norm": 0.3260982930660248,
      "learning_rate": 2.3602484472049692e-05,
      "loss": 1.7281,
      "step": 77
    },
    {
      "epoch": 0.14606741573033707,
      "grad_norm": 0.36218687891960144,
      "learning_rate": 2.391304347826087e-05,
      "loss": 1.764,
      "step": 78
    },
    {
      "epoch": 0.14794007490636704,
      "grad_norm": 0.3543208837509155,
      "learning_rate": 2.4223602484472053e-05,
      "loss": 1.9096,
      "step": 79
    },
    {
      "epoch": 0.149812734082397,
      "grad_norm": 0.3399237096309662,
      "learning_rate": 2.453416149068323e-05,
      "loss": 1.7556,
      "step": 80
    },
    {
      "epoch": 0.15168539325842698,
      "grad_norm": 0.42604950070381165,
      "learning_rate": 2.484472049689441e-05,
      "loss": 1.7544,
      "step": 81
    },
    {
      "epoch": 0.15355805243445692,
      "grad_norm": 0.40462759137153625,
      "learning_rate": 2.515527950310559e-05,
      "loss": 1.7999,
      "step": 82
    },
    {
      "epoch": 0.15543071161048688,
      "grad_norm": 0.354352205991745,
      "learning_rate": 2.546583850931677e-05,
      "loss": 1.7281,
      "step": 83
    },
    {
      "epoch": 0.15730337078651685,
      "grad_norm": 0.3782781958580017,
      "learning_rate": 2.577639751552795e-05,
      "loss": 1.8337,
      "step": 84
    },
    {
      "epoch": 0.15917602996254682,
      "grad_norm": 0.3312382996082306,
      "learning_rate": 2.608695652173913e-05,
      "loss": 1.8487,
      "step": 85
    },
    {
      "epoch": 0.16104868913857678,
      "grad_norm": 0.39018726348876953,
      "learning_rate": 2.639751552795031e-05,
      "loss": 1.8481,
      "step": 86
    },
    {
      "epoch": 0.16292134831460675,
      "grad_norm": 0.359090656042099,
      "learning_rate": 2.6708074534161492e-05,
      "loss": 1.8024,
      "step": 87
    },
    {
      "epoch": 0.1647940074906367,
      "grad_norm": 0.3985334038734436,
      "learning_rate": 2.7018633540372674e-05,
      "loss": 1.729,
      "step": 88
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 0.40672048926353455,
      "learning_rate": 2.7329192546583853e-05,
      "loss": 1.8649,
      "step": 89
    },
    {
      "epoch": 0.16853932584269662,
      "grad_norm": 0.35837894678115845,
      "learning_rate": 2.7639751552795035e-05,
      "loss": 1.768,
      "step": 90
    },
    {
      "epoch": 0.1704119850187266,
      "grad_norm": 0.402141273021698,
      "learning_rate": 2.795031055900621e-05,
      "loss": 1.7136,
      "step": 91
    },
    {
      "epoch": 0.17228464419475656,
      "grad_norm": 0.3584944009780884,
      "learning_rate": 2.826086956521739e-05,
      "loss": 1.7941,
      "step": 92
    },
    {
      "epoch": 0.17415730337078653,
      "grad_norm": 0.3910036087036133,
      "learning_rate": 2.857142857142857e-05,
      "loss": 1.857,
      "step": 93
    },
    {
      "epoch": 0.1760299625468165,
      "grad_norm": 0.36750155687332153,
      "learning_rate": 2.8881987577639753e-05,
      "loss": 1.8726,
      "step": 94
    },
    {
      "epoch": 0.17790262172284643,
      "grad_norm": 0.3793822228908539,
      "learning_rate": 2.919254658385093e-05,
      "loss": 1.7261,
      "step": 95
    },
    {
      "epoch": 0.1797752808988764,
      "grad_norm": 0.34727659821510315,
      "learning_rate": 2.9503105590062114e-05,
      "loss": 1.7811,
      "step": 96
    },
    {
      "epoch": 0.18164794007490637,
      "grad_norm": 0.37734299898147583,
      "learning_rate": 2.9813664596273296e-05,
      "loss": 1.7769,
      "step": 97
    },
    {
      "epoch": 0.18352059925093633,
      "grad_norm": 0.39186421036720276,
      "learning_rate": 3.0124223602484474e-05,
      "loss": 1.7775,
      "step": 98
    },
    {
      "epoch": 0.1853932584269663,
      "grad_norm": 0.3800007402896881,
      "learning_rate": 3.0434782608695656e-05,
      "loss": 1.7564,
      "step": 99
    },
    {
      "epoch": 0.18726591760299627,
      "grad_norm": 0.3335358500480652,
      "learning_rate": 3.074534161490684e-05,
      "loss": 1.7873,
      "step": 100
    },
    {
      "epoch": 0.1891385767790262,
      "grad_norm": 0.37501880526542664,
      "learning_rate": 3.1055900621118014e-05,
      "loss": 1.6795,
      "step": 101
    },
    {
      "epoch": 0.19101123595505617,
      "grad_norm": 0.389769583940506,
      "learning_rate": 3.136645962732919e-05,
      "loss": 1.6774,
      "step": 102
    },
    {
      "epoch": 0.19288389513108614,
      "grad_norm": 0.41609570384025574,
      "learning_rate": 3.167701863354037e-05,
      "loss": 1.855,
      "step": 103
    },
    {
      "epoch": 0.1947565543071161,
      "grad_norm": 0.3538341820240021,
      "learning_rate": 3.198757763975155e-05,
      "loss": 1.7056,
      "step": 104
    },
    {
      "epoch": 0.19662921348314608,
      "grad_norm": 0.4053025543689728,
      "learning_rate": 3.2298136645962735e-05,
      "loss": 1.8451,
      "step": 105
    },
    {
      "epoch": 0.19850187265917604,
      "grad_norm": 0.33943474292755127,
      "learning_rate": 3.260869565217392e-05,
      "loss": 1.7755,
      "step": 106
    },
    {
      "epoch": 0.20037453183520598,
      "grad_norm": 0.36902445554733276,
      "learning_rate": 3.29192546583851e-05,
      "loss": 1.7587,
      "step": 107
    },
    {
      "epoch": 0.20224719101123595,
      "grad_norm": 0.3978821933269501,
      "learning_rate": 3.3229813664596274e-05,
      "loss": 1.892,
      "step": 108
    },
    {
      "epoch": 0.20411985018726592,
      "grad_norm": 0.37120071053504944,
      "learning_rate": 3.3540372670807456e-05,
      "loss": 1.6586,
      "step": 109
    },
    {
      "epoch": 0.20599250936329588,
      "grad_norm": 0.36096087098121643,
      "learning_rate": 3.385093167701863e-05,
      "loss": 1.6094,
      "step": 110
    },
    {
      "epoch": 0.20786516853932585,
      "grad_norm": 0.42157644033432007,
      "learning_rate": 3.4161490683229814e-05,
      "loss": 1.844,
      "step": 111
    },
    {
      "epoch": 0.20973782771535582,
      "grad_norm": 0.3952377438545227,
      "learning_rate": 3.4472049689440996e-05,
      "loss": 1.7602,
      "step": 112
    },
    {
      "epoch": 0.21161048689138576,
      "grad_norm": 0.3910966217517853,
      "learning_rate": 3.478260869565218e-05,
      "loss": 1.8141,
      "step": 113
    },
    {
      "epoch": 0.21348314606741572,
      "grad_norm": 0.3990914523601532,
      "learning_rate": 3.509316770186335e-05,
      "loss": 1.8017,
      "step": 114
    },
    {
      "epoch": 0.2153558052434457,
      "grad_norm": 0.41213494539260864,
      "learning_rate": 3.5403726708074535e-05,
      "loss": 1.6948,
      "step": 115
    },
    {
      "epoch": 0.21722846441947566,
      "grad_norm": 0.38012874126434326,
      "learning_rate": 3.571428571428572e-05,
      "loss": 1.7006,
      "step": 116
    },
    {
      "epoch": 0.21910112359550563,
      "grad_norm": 0.3811723589897156,
      "learning_rate": 3.60248447204969e-05,
      "loss": 1.7021,
      "step": 117
    },
    {
      "epoch": 0.2209737827715356,
      "grad_norm": 0.42865946888923645,
      "learning_rate": 3.633540372670808e-05,
      "loss": 1.6299,
      "step": 118
    },
    {
      "epoch": 0.22284644194756553,
      "grad_norm": 0.4026777148246765,
      "learning_rate": 3.6645962732919256e-05,
      "loss": 1.7612,
      "step": 119
    },
    {
      "epoch": 0.2247191011235955,
      "grad_norm": 0.42247912287712097,
      "learning_rate": 3.695652173913043e-05,
      "loss": 1.7409,
      "step": 120
    },
    {
      "epoch": 0.22659176029962547,
      "grad_norm": 0.42985397577285767,
      "learning_rate": 3.7267080745341614e-05,
      "loss": 1.7313,
      "step": 121
    },
    {
      "epoch": 0.22846441947565543,
      "grad_norm": 0.4755229651927948,
      "learning_rate": 3.7577639751552796e-05,
      "loss": 1.7346,
      "step": 122
    },
    {
      "epoch": 0.2303370786516854,
      "grad_norm": 0.42776182293891907,
      "learning_rate": 3.788819875776398e-05,
      "loss": 1.7873,
      "step": 123
    },
    {
      "epoch": 0.23220973782771537,
      "grad_norm": 0.3842492699623108,
      "learning_rate": 3.819875776397516e-05,
      "loss": 1.7104,
      "step": 124
    },
    {
      "epoch": 0.2340823970037453,
      "grad_norm": 0.4553511440753937,
      "learning_rate": 3.8509316770186335e-05,
      "loss": 1.7492,
      "step": 125
    },
    {
      "epoch": 0.23595505617977527,
      "grad_norm": 0.42437711358070374,
      "learning_rate": 3.881987577639752e-05,
      "loss": 1.8246,
      "step": 126
    },
    {
      "epoch": 0.23782771535580524,
      "grad_norm": 0.4739566147327423,
      "learning_rate": 3.91304347826087e-05,
      "loss": 1.6037,
      "step": 127
    },
    {
      "epoch": 0.2397003745318352,
      "grad_norm": 0.4193587601184845,
      "learning_rate": 3.944099378881988e-05,
      "loss": 1.699,
      "step": 128
    },
    {
      "epoch": 0.24157303370786518,
      "grad_norm": 0.3803868293762207,
      "learning_rate": 3.9751552795031056e-05,
      "loss": 1.6949,
      "step": 129
    },
    {
      "epoch": 0.24344569288389514,
      "grad_norm": 0.4742130637168884,
      "learning_rate": 4.006211180124224e-05,
      "loss": 1.7066,
      "step": 130
    },
    {
      "epoch": 0.24531835205992508,
      "grad_norm": 0.47418415546417236,
      "learning_rate": 4.0372670807453414e-05,
      "loss": 1.7475,
      "step": 131
    },
    {
      "epoch": 0.24719101123595505,
      "grad_norm": 0.5409931540489197,
      "learning_rate": 4.0683229813664596e-05,
      "loss": 1.7507,
      "step": 132
    },
    {
      "epoch": 0.24906367041198502,
      "grad_norm": 0.4977431297302246,
      "learning_rate": 4.099378881987578e-05,
      "loss": 1.6875,
      "step": 133
    },
    {
      "epoch": 0.250936329588015,
      "grad_norm": 0.4619319438934326,
      "learning_rate": 4.130434782608696e-05,
      "loss": 1.7027,
      "step": 134
    },
    {
      "epoch": 0.25280898876404495,
      "grad_norm": 0.49388429522514343,
      "learning_rate": 4.161490683229814e-05,
      "loss": 1.6684,
      "step": 135
    },
    {
      "epoch": 0.2546816479400749,
      "grad_norm": 0.492258757352829,
      "learning_rate": 4.192546583850932e-05,
      "loss": 1.613,
      "step": 136
    },
    {
      "epoch": 0.2565543071161049,
      "grad_norm": 0.5866583585739136,
      "learning_rate": 4.22360248447205e-05,
      "loss": 1.7348,
      "step": 137
    },
    {
      "epoch": 0.25842696629213485,
      "grad_norm": 0.5072633624076843,
      "learning_rate": 4.254658385093168e-05,
      "loss": 1.7818,
      "step": 138
    },
    {
      "epoch": 0.2602996254681648,
      "grad_norm": 0.5223544836044312,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 1.6851,
      "step": 139
    },
    {
      "epoch": 0.26217228464419473,
      "grad_norm": 0.4360044300556183,
      "learning_rate": 4.316770186335404e-05,
      "loss": 1.7957,
      "step": 140
    },
    {
      "epoch": 0.2640449438202247,
      "grad_norm": 0.44919753074645996,
      "learning_rate": 4.347826086956522e-05,
      "loss": 1.7487,
      "step": 141
    },
    {
      "epoch": 0.26591760299625467,
      "grad_norm": 0.42496204376220703,
      "learning_rate": 4.3788819875776396e-05,
      "loss": 1.5243,
      "step": 142
    },
    {
      "epoch": 0.26779026217228463,
      "grad_norm": 0.42837780714035034,
      "learning_rate": 4.409937888198758e-05,
      "loss": 1.7217,
      "step": 143
    },
    {
      "epoch": 0.2696629213483146,
      "grad_norm": 0.44026824831962585,
      "learning_rate": 4.440993788819876e-05,
      "loss": 1.6845,
      "step": 144
    },
    {
      "epoch": 0.27153558052434457,
      "grad_norm": 0.511133074760437,
      "learning_rate": 4.472049689440994e-05,
      "loss": 1.7168,
      "step": 145
    },
    {
      "epoch": 0.27340823970037453,
      "grad_norm": 0.45767247676849365,
      "learning_rate": 4.5031055900621124e-05,
      "loss": 1.6815,
      "step": 146
    },
    {
      "epoch": 0.2752808988764045,
      "grad_norm": 0.3954430818557739,
      "learning_rate": 4.5341614906832306e-05,
      "loss": 1.7262,
      "step": 147
    },
    {
      "epoch": 0.27715355805243447,
      "grad_norm": 0.4350290298461914,
      "learning_rate": 4.565217391304348e-05,
      "loss": 1.785,
      "step": 148
    },
    {
      "epoch": 0.27902621722846443,
      "grad_norm": 0.4579932987689972,
      "learning_rate": 4.5962732919254656e-05,
      "loss": 1.6197,
      "step": 149
    },
    {
      "epoch": 0.2808988764044944,
      "grad_norm": 0.43843284249305725,
      "learning_rate": 4.627329192546584e-05,
      "loss": 1.5389,
      "step": 150
    },
    {
      "epoch": 0.28277153558052437,
      "grad_norm": 0.4474106729030609,
      "learning_rate": 4.658385093167702e-05,
      "loss": 1.6255,
      "step": 151
    },
    {
      "epoch": 0.2846441947565543,
      "grad_norm": 0.4412067234516144,
      "learning_rate": 4.68944099378882e-05,
      "loss": 1.6834,
      "step": 152
    },
    {
      "epoch": 0.28651685393258425,
      "grad_norm": 0.4506111443042755,
      "learning_rate": 4.7204968944099384e-05,
      "loss": 1.4942,
      "step": 153
    },
    {
      "epoch": 0.2883895131086142,
      "grad_norm": 0.46009063720703125,
      "learning_rate": 4.751552795031056e-05,
      "loss": 1.7175,
      "step": 154
    },
    {
      "epoch": 0.2902621722846442,
      "grad_norm": 0.4788944125175476,
      "learning_rate": 4.782608695652174e-05,
      "loss": 1.5627,
      "step": 155
    },
    {
      "epoch": 0.29213483146067415,
      "grad_norm": 0.4873255491256714,
      "learning_rate": 4.8136645962732924e-05,
      "loss": 1.6303,
      "step": 156
    },
    {
      "epoch": 0.2940074906367041,
      "grad_norm": 0.5658560991287231,
      "learning_rate": 4.8447204968944106e-05,
      "loss": 1.5361,
      "step": 157
    },
    {
      "epoch": 0.2958801498127341,
      "grad_norm": 0.515152633190155,
      "learning_rate": 4.875776397515528e-05,
      "loss": 1.6535,
      "step": 158
    },
    {
      "epoch": 0.29775280898876405,
      "grad_norm": 0.4952266216278076,
      "learning_rate": 4.906832298136646e-05,
      "loss": 1.6764,
      "step": 159
    },
    {
      "epoch": 0.299625468164794,
      "grad_norm": 0.6188328266143799,
      "learning_rate": 4.937888198757764e-05,
      "loss": 1.599,
      "step": 160
    },
    {
      "epoch": 0.301498127340824,
      "grad_norm": 0.48562610149383545,
      "learning_rate": 4.968944099378882e-05,
      "loss": 1.6437,
      "step": 161
    },
    {
      "epoch": 0.30337078651685395,
      "grad_norm": 0.5951073169708252,
      "learning_rate": 5e-05,
      "loss": 1.6303,
      "step": 162
    },
    {
      "epoch": 0.3052434456928839,
      "grad_norm": 0.529066264629364,
      "learning_rate": 4.9999940586980495e-05,
      "loss": 1.7076,
      "step": 163
    },
    {
      "epoch": 0.30711610486891383,
      "grad_norm": 0.48726004362106323,
      "learning_rate": 4.999976234820439e-05,
      "loss": 1.6347,
      "step": 164
    },
    {
      "epoch": 0.3089887640449438,
      "grad_norm": 0.5474575757980347,
      "learning_rate": 4.999946528451884e-05,
      "loss": 1.5671,
      "step": 165
    },
    {
      "epoch": 0.31086142322097376,
      "grad_norm": 0.5025075674057007,
      "learning_rate": 4.999904939733581e-05,
      "loss": 1.563,
      "step": 166
    },
    {
      "epoch": 0.31273408239700373,
      "grad_norm": 0.4056752026081085,
      "learning_rate": 4.999851468863204e-05,
      "loss": 1.6355,
      "step": 167
    },
    {
      "epoch": 0.3146067415730337,
      "grad_norm": 0.4447513222694397,
      "learning_rate": 4.9997861160949e-05,
      "loss": 1.5882,
      "step": 168
    },
    {
      "epoch": 0.31647940074906367,
      "grad_norm": 0.4530852138996124,
      "learning_rate": 4.999708881739296e-05,
      "loss": 1.6601,
      "step": 169
    },
    {
      "epoch": 0.31835205992509363,
      "grad_norm": 0.4419321119785309,
      "learning_rate": 4.9996197661634876e-05,
      "loss": 1.5467,
      "step": 170
    },
    {
      "epoch": 0.3202247191011236,
      "grad_norm": 0.4942760169506073,
      "learning_rate": 4.999518769791046e-05,
      "loss": 1.6047,
      "step": 171
    },
    {
      "epoch": 0.32209737827715357,
      "grad_norm": 0.489688515663147,
      "learning_rate": 4.999405893102012e-05,
      "loss": 1.6558,
      "step": 172
    },
    {
      "epoch": 0.32397003745318353,
      "grad_norm": 0.4632798731327057,
      "learning_rate": 4.9992811366328926e-05,
      "loss": 1.7385,
      "step": 173
    },
    {
      "epoch": 0.3258426966292135,
      "grad_norm": 0.46135276556015015,
      "learning_rate": 4.9991445009766595e-05,
      "loss": 1.5566,
      "step": 174
    },
    {
      "epoch": 0.32771535580524347,
      "grad_norm": 0.4911576807498932,
      "learning_rate": 4.9989959867827486e-05,
      "loss": 1.5602,
      "step": 175
    },
    {
      "epoch": 0.3295880149812734,
      "grad_norm": 0.47521939873695374,
      "learning_rate": 4.998835594757054e-05,
      "loss": 1.6099,
      "step": 176
    },
    {
      "epoch": 0.33146067415730335,
      "grad_norm": 0.4398667514324188,
      "learning_rate": 4.998663325661926e-05,
      "loss": 1.6729,
      "step": 177
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.4557138979434967,
      "learning_rate": 4.998479180316166e-05,
      "loss": 1.6838,
      "step": 178
    },
    {
      "epoch": 0.3352059925093633,
      "grad_norm": 0.4194803237915039,
      "learning_rate": 4.998283159595024e-05,
      "loss": 1.5801,
      "step": 179
    },
    {
      "epoch": 0.33707865168539325,
      "grad_norm": 0.4040605425834656,
      "learning_rate": 4.9980752644301964e-05,
      "loss": 1.5436,
      "step": 180
    },
    {
      "epoch": 0.3389513108614232,
      "grad_norm": 0.48027148842811584,
      "learning_rate": 4.997855495809816e-05,
      "loss": 1.6818,
      "step": 181
    },
    {
      "epoch": 0.3408239700374532,
      "grad_norm": 0.45877721905708313,
      "learning_rate": 4.997623854778453e-05,
      "loss": 1.6829,
      "step": 182
    },
    {
      "epoch": 0.34269662921348315,
      "grad_norm": 0.4706817865371704,
      "learning_rate": 4.997380342437107e-05,
      "loss": 1.6626,
      "step": 183
    },
    {
      "epoch": 0.3445692883895131,
      "grad_norm": 0.4681834578514099,
      "learning_rate": 4.997124959943201e-05,
      "loss": 1.6517,
      "step": 184
    },
    {
      "epoch": 0.3464419475655431,
      "grad_norm": 0.4642466902732849,
      "learning_rate": 4.99685770851058e-05,
      "loss": 1.5649,
      "step": 185
    },
    {
      "epoch": 0.34831460674157305,
      "grad_norm": 0.47761133313179016,
      "learning_rate": 4.996578589409501e-05,
      "loss": 1.4864,
      "step": 186
    },
    {
      "epoch": 0.350187265917603,
      "grad_norm": 0.4350415766239166,
      "learning_rate": 4.996287603966627e-05,
      "loss": 1.5827,
      "step": 187
    },
    {
      "epoch": 0.352059925093633,
      "grad_norm": 0.4305781424045563,
      "learning_rate": 4.995984753565027e-05,
      "loss": 1.6798,
      "step": 188
    },
    {
      "epoch": 0.3539325842696629,
      "grad_norm": 0.5154985189437866,
      "learning_rate": 4.995670039644159e-05,
      "loss": 1.6617,
      "step": 189
    },
    {
      "epoch": 0.35580524344569286,
      "grad_norm": 0.4289360046386719,
      "learning_rate": 4.995343463699872e-05,
      "loss": 1.671,
      "step": 190
    },
    {
      "epoch": 0.35767790262172283,
      "grad_norm": 0.47543489933013916,
      "learning_rate": 4.9950050272843954e-05,
      "loss": 1.5718,
      "step": 191
    },
    {
      "epoch": 0.3595505617977528,
      "grad_norm": 0.4646323621273041,
      "learning_rate": 4.994654732006331e-05,
      "loss": 1.6119,
      "step": 192
    },
    {
      "epoch": 0.36142322097378277,
      "grad_norm": 0.47146928310394287,
      "learning_rate": 4.9942925795306476e-05,
      "loss": 1.5146,
      "step": 193
    },
    {
      "epoch": 0.36329588014981273,
      "grad_norm": 0.46810752153396606,
      "learning_rate": 4.993918571578671e-05,
      "loss": 1.4421,
      "step": 194
    },
    {
      "epoch": 0.3651685393258427,
      "grad_norm": 0.4814688563346863,
      "learning_rate": 4.993532709928075e-05,
      "loss": 1.5877,
      "step": 195
    },
    {
      "epoch": 0.36704119850187267,
      "grad_norm": 0.4733228385448456,
      "learning_rate": 4.993134996412877e-05,
      "loss": 1.7494,
      "step": 196
    },
    {
      "epoch": 0.36891385767790263,
      "grad_norm": 0.4503585696220398,
      "learning_rate": 4.9927254329234266e-05,
      "loss": 1.5383,
      "step": 197
    },
    {
      "epoch": 0.3707865168539326,
      "grad_norm": 0.47338247299194336,
      "learning_rate": 4.9923040214063954e-05,
      "loss": 1.5499,
      "step": 198
    },
    {
      "epoch": 0.37265917602996257,
      "grad_norm": 0.5167230367660522,
      "learning_rate": 4.99187076386477e-05,
      "loss": 1.6594,
      "step": 199
    },
    {
      "epoch": 0.37453183520599254,
      "grad_norm": 0.43341049551963806,
      "learning_rate": 4.991425662357841e-05,
      "loss": 1.5948,
      "step": 200
    },
    {
      "epoch": 0.37640449438202245,
      "grad_norm": 0.4583638906478882,
      "learning_rate": 4.990968719001195e-05,
      "loss": 1.5639,
      "step": 201
    },
    {
      "epoch": 0.3782771535580524,
      "grad_norm": 0.46908795833587646,
      "learning_rate": 4.990499935966702e-05,
      "loss": 1.7046,
      "step": 202
    },
    {
      "epoch": 0.3801498127340824,
      "grad_norm": 0.4234890937805176,
      "learning_rate": 4.9900193154825095e-05,
      "loss": 1.6565,
      "step": 203
    },
    {
      "epoch": 0.38202247191011235,
      "grad_norm": 0.468651682138443,
      "learning_rate": 4.989526859833024e-05,
      "loss": 1.5565,
      "step": 204
    },
    {
      "epoch": 0.3838951310861423,
      "grad_norm": 0.4852917492389679,
      "learning_rate": 4.989022571358908e-05,
      "loss": 1.5939,
      "step": 205
    },
    {
      "epoch": 0.3857677902621723,
      "grad_norm": 0.48728272318840027,
      "learning_rate": 4.9885064524570665e-05,
      "loss": 1.6171,
      "step": 206
    },
    {
      "epoch": 0.38764044943820225,
      "grad_norm": 0.4841589629650116,
      "learning_rate": 4.987978505580634e-05,
      "loss": 1.7535,
      "step": 207
    },
    {
      "epoch": 0.3895131086142322,
      "grad_norm": 0.44958075881004333,
      "learning_rate": 4.987438733238963e-05,
      "loss": 1.6397,
      "step": 208
    },
    {
      "epoch": 0.3913857677902622,
      "grad_norm": 0.40982890129089355,
      "learning_rate": 4.986887137997615e-05,
      "loss": 1.5916,
      "step": 209
    },
    {
      "epoch": 0.39325842696629215,
      "grad_norm": 0.4669470191001892,
      "learning_rate": 4.986323722478345e-05,
      "loss": 1.6152,
      "step": 210
    },
    {
      "epoch": 0.3951310861423221,
      "grad_norm": 0.46840542554855347,
      "learning_rate": 4.98574848935909e-05,
      "loss": 1.6067,
      "step": 211
    },
    {
      "epoch": 0.3970037453183521,
      "grad_norm": 0.4363192915916443,
      "learning_rate": 4.9851614413739566e-05,
      "loss": 1.4181,
      "step": 212
    },
    {
      "epoch": 0.398876404494382,
      "grad_norm": 0.44868770241737366,
      "learning_rate": 4.984562581313209e-05,
      "loss": 1.617,
      "step": 213
    },
    {
      "epoch": 0.40074906367041196,
      "grad_norm": 0.47699087858200073,
      "learning_rate": 4.9839519120232534e-05,
      "loss": 1.5516,
      "step": 214
    },
    {
      "epoch": 0.40262172284644193,
      "grad_norm": 0.43826696276664734,
      "learning_rate": 4.9833294364066266e-05,
      "loss": 1.4824,
      "step": 215
    },
    {
      "epoch": 0.4044943820224719,
      "grad_norm": 0.47640207409858704,
      "learning_rate": 4.982695157421982e-05,
      "loss": 1.6668,
      "step": 216
    },
    {
      "epoch": 0.40636704119850187,
      "grad_norm": 0.45764973759651184,
      "learning_rate": 4.982049078084071e-05,
      "loss": 1.4296,
      "step": 217
    },
    {
      "epoch": 0.40823970037453183,
      "grad_norm": 0.5040321350097656,
      "learning_rate": 4.981391201463739e-05,
      "loss": 1.6663,
      "step": 218
    },
    {
      "epoch": 0.4101123595505618,
      "grad_norm": 0.4582141637802124,
      "learning_rate": 4.980721530687899e-05,
      "loss": 1.7371,
      "step": 219
    },
    {
      "epoch": 0.41198501872659177,
      "grad_norm": 0.47192853689193726,
      "learning_rate": 4.980040068939524e-05,
      "loss": 1.47,
      "step": 220
    },
    {
      "epoch": 0.41385767790262173,
      "grad_norm": 0.4862860143184662,
      "learning_rate": 4.979346819457631e-05,
      "loss": 1.6806,
      "step": 221
    },
    {
      "epoch": 0.4157303370786517,
      "grad_norm": 0.46545830368995667,
      "learning_rate": 4.978641785537264e-05,
      "loss": 1.5947,
      "step": 222
    },
    {
      "epoch": 0.41760299625468167,
      "grad_norm": 0.5318657755851746,
      "learning_rate": 4.9779249705294764e-05,
      "loss": 1.5668,
      "step": 223
    },
    {
      "epoch": 0.41947565543071164,
      "grad_norm": 0.48563045263290405,
      "learning_rate": 4.977196377841321e-05,
      "loss": 1.7286,
      "step": 224
    },
    {
      "epoch": 0.42134831460674155,
      "grad_norm": 0.5101299285888672,
      "learning_rate": 4.9764560109358295e-05,
      "loss": 1.6347,
      "step": 225
    },
    {
      "epoch": 0.4232209737827715,
      "grad_norm": 0.5359089970588684,
      "learning_rate": 4.975703873331996e-05,
      "loss": 1.5182,
      "step": 226
    },
    {
      "epoch": 0.4250936329588015,
      "grad_norm": 0.5270504355430603,
      "learning_rate": 4.974939968604761e-05,
      "loss": 1.5827,
      "step": 227
    },
    {
      "epoch": 0.42696629213483145,
      "grad_norm": 0.44772931933403015,
      "learning_rate": 4.974164300384998e-05,
      "loss": 1.5263,
      "step": 228
    },
    {
      "epoch": 0.4288389513108614,
      "grad_norm": 0.48192083835601807,
      "learning_rate": 4.973376872359488e-05,
      "loss": 1.5895,
      "step": 229
    },
    {
      "epoch": 0.4307116104868914,
      "grad_norm": 0.45213600993156433,
      "learning_rate": 4.972577688270909e-05,
      "loss": 1.6639,
      "step": 230
    },
    {
      "epoch": 0.43258426966292135,
      "grad_norm": 0.45784851908683777,
      "learning_rate": 4.9717667519178176e-05,
      "loss": 1.5234,
      "step": 231
    },
    {
      "epoch": 0.4344569288389513,
      "grad_norm": 0.4590270519256592,
      "learning_rate": 4.970944067154627e-05,
      "loss": 1.6238,
      "step": 232
    },
    {
      "epoch": 0.4363295880149813,
      "grad_norm": 0.5344392657279968,
      "learning_rate": 4.970109637891593e-05,
      "loss": 1.5518,
      "step": 233
    },
    {
      "epoch": 0.43820224719101125,
      "grad_norm": 0.4633795917034149,
      "learning_rate": 4.969263468094791e-05,
      "loss": 1.6347,
      "step": 234
    },
    {
      "epoch": 0.4400749063670412,
      "grad_norm": 0.5235575437545776,
      "learning_rate": 4.968405561786103e-05,
      "loss": 1.6521,
      "step": 235
    },
    {
      "epoch": 0.4419475655430712,
      "grad_norm": 0.5151293873786926,
      "learning_rate": 4.967535923043192e-05,
      "loss": 1.5072,
      "step": 236
    },
    {
      "epoch": 0.4438202247191011,
      "grad_norm": 0.5542312264442444,
      "learning_rate": 4.966654555999488e-05,
      "loss": 1.5844,
      "step": 237
    },
    {
      "epoch": 0.44569288389513106,
      "grad_norm": 0.45711588859558105,
      "learning_rate": 4.965761464844165e-05,
      "loss": 1.5816,
      "step": 238
    },
    {
      "epoch": 0.44756554307116103,
      "grad_norm": 0.464353084564209,
      "learning_rate": 4.964856653822122e-05,
      "loss": 1.5668,
      "step": 239
    },
    {
      "epoch": 0.449438202247191,
      "grad_norm": 0.4691389203071594,
      "learning_rate": 4.963940127233963e-05,
      "loss": 1.653,
      "step": 240
    },
    {
      "epoch": 0.45131086142322097,
      "grad_norm": 0.5310739278793335,
      "learning_rate": 4.9630118894359775e-05,
      "loss": 1.6086,
      "step": 241
    },
    {
      "epoch": 0.45318352059925093,
      "grad_norm": 0.4655667841434479,
      "learning_rate": 4.962071944840119e-05,
      "loss": 1.6367,
      "step": 242
    },
    {
      "epoch": 0.4550561797752809,
      "grad_norm": 0.5003381371498108,
      "learning_rate": 4.961120297913983e-05,
      "loss": 1.5426,
      "step": 243
    },
    {
      "epoch": 0.45692883895131087,
      "grad_norm": 0.506240963935852,
      "learning_rate": 4.960156953180786e-05,
      "loss": 1.6613,
      "step": 244
    },
    {
      "epoch": 0.45880149812734083,
      "grad_norm": 0.4841075837612152,
      "learning_rate": 4.959181915219345e-05,
      "loss": 1.6577,
      "step": 245
    },
    {
      "epoch": 0.4606741573033708,
      "grad_norm": 0.48877766728401184,
      "learning_rate": 4.958195188664058e-05,
      "loss": 1.4769,
      "step": 246
    },
    {
      "epoch": 0.46254681647940077,
      "grad_norm": 0.548494815826416,
      "learning_rate": 4.9571967782048765e-05,
      "loss": 1.6574,
      "step": 247
    },
    {
      "epoch": 0.46441947565543074,
      "grad_norm": 0.4717058837413788,
      "learning_rate": 4.956186688587287e-05,
      "loss": 1.5974,
      "step": 248
    },
    {
      "epoch": 0.46629213483146065,
      "grad_norm": 0.5176948308944702,
      "learning_rate": 4.9551649246122854e-05,
      "loss": 1.5454,
      "step": 249
    },
    {
      "epoch": 0.4681647940074906,
      "grad_norm": 0.5062636137008667,
      "learning_rate": 4.954131491136362e-05,
      "loss": 1.4789,
      "step": 250
    },
    {
      "epoch": 0.4700374531835206,
      "grad_norm": 0.546802818775177,
      "learning_rate": 4.953086393071466e-05,
      "loss": 1.6565,
      "step": 251
    },
    {
      "epoch": 0.47191011235955055,
      "grad_norm": 0.4690517783164978,
      "learning_rate": 4.9520296353849935e-05,
      "loss": 1.4606,
      "step": 252
    },
    {
      "epoch": 0.4737827715355805,
      "grad_norm": 0.49683260917663574,
      "learning_rate": 4.950961223099757e-05,
      "loss": 1.4763,
      "step": 253
    },
    {
      "epoch": 0.4756554307116105,
      "grad_norm": 0.5106662511825562,
      "learning_rate": 4.9498811612939656e-05,
      "loss": 1.6392,
      "step": 254
    },
    {
      "epoch": 0.47752808988764045,
      "grad_norm": 0.47190266847610474,
      "learning_rate": 4.948789455101196e-05,
      "loss": 1.608,
      "step": 255
    },
    {
      "epoch": 0.4794007490636704,
      "grad_norm": 0.524330198764801,
      "learning_rate": 4.947686109710375e-05,
      "loss": 1.6629,
      "step": 256
    },
    {
      "epoch": 0.4812734082397004,
      "grad_norm": 0.5221540927886963,
      "learning_rate": 4.946571130365748e-05,
      "loss": 1.5455,
      "step": 257
    },
    {
      "epoch": 0.48314606741573035,
      "grad_norm": 0.4754229485988617,
      "learning_rate": 4.9454445223668586e-05,
      "loss": 1.5989,
      "step": 258
    },
    {
      "epoch": 0.4850187265917603,
      "grad_norm": 0.4658515155315399,
      "learning_rate": 4.944306291068522e-05,
      "loss": 1.5531,
      "step": 259
    },
    {
      "epoch": 0.4868913857677903,
      "grad_norm": 0.5560185313224792,
      "learning_rate": 4.943156441880797e-05,
      "loss": 1.7779,
      "step": 260
    },
    {
      "epoch": 0.4887640449438202,
      "grad_norm": 0.5857990384101868,
      "learning_rate": 4.9419949802689666e-05,
      "loss": 1.4988,
      "step": 261
    },
    {
      "epoch": 0.49063670411985016,
      "grad_norm": 0.46129927039146423,
      "learning_rate": 4.940821911753505e-05,
      "loss": 1.5967,
      "step": 262
    },
    {
      "epoch": 0.49250936329588013,
      "grad_norm": 0.5320917963981628,
      "learning_rate": 4.939637241910056e-05,
      "loss": 1.5447,
      "step": 263
    },
    {
      "epoch": 0.4943820224719101,
      "grad_norm": 0.4441697299480438,
      "learning_rate": 4.9384409763694045e-05,
      "loss": 1.5534,
      "step": 264
    },
    {
      "epoch": 0.49625468164794007,
      "grad_norm": 0.49320903420448303,
      "learning_rate": 4.937233120817451e-05,
      "loss": 1.5241,
      "step": 265
    },
    {
      "epoch": 0.49812734082397003,
      "grad_norm": 0.4489322304725647,
      "learning_rate": 4.936013680995181e-05,
      "loss": 1.5354,
      "step": 266
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.467287540435791,
      "learning_rate": 4.9347826626986445e-05,
      "loss": 1.5562,
      "step": 267
    },
    {
      "epoch": 0.50187265917603,
      "grad_norm": 0.5086188316345215,
      "learning_rate": 4.9335400717789233e-05,
      "loss": 1.5237,
      "step": 268
    },
    {
      "epoch": 0.5037453183520599,
      "grad_norm": 0.48977747559547424,
      "learning_rate": 4.932285914142102e-05,
      "loss": 1.5825,
      "step": 269
    },
    {
      "epoch": 0.5056179775280899,
      "grad_norm": 0.4953845143318176,
      "learning_rate": 4.9310201957492444e-05,
      "loss": 1.5372,
      "step": 270
    },
    {
      "epoch": 0.5074906367041199,
      "grad_norm": 0.5320143103599548,
      "learning_rate": 4.929742922616363e-05,
      "loss": 1.5643,
      "step": 271
    },
    {
      "epoch": 0.5093632958801498,
      "grad_norm": 0.4930180609226227,
      "learning_rate": 4.92845410081439e-05,
      "loss": 1.5344,
      "step": 272
    },
    {
      "epoch": 0.5112359550561798,
      "grad_norm": 0.5611414909362793,
      "learning_rate": 4.927153736469149e-05,
      "loss": 1.6408,
      "step": 273
    },
    {
      "epoch": 0.5131086142322098,
      "grad_norm": 0.47521713376045227,
      "learning_rate": 4.9258418357613257e-05,
      "loss": 1.5893,
      "step": 274
    },
    {
      "epoch": 0.5149812734082397,
      "grad_norm": 0.5755829215049744,
      "learning_rate": 4.924518404926438e-05,
      "loss": 1.4687,
      "step": 275
    },
    {
      "epoch": 0.5168539325842697,
      "grad_norm": 0.5123838186264038,
      "learning_rate": 4.923183450254809e-05,
      "loss": 1.6199,
      "step": 276
    },
    {
      "epoch": 0.5187265917602997,
      "grad_norm": 0.48199954628944397,
      "learning_rate": 4.921836978091533e-05,
      "loss": 1.57,
      "step": 277
    },
    {
      "epoch": 0.5205992509363296,
      "grad_norm": 0.4791603982448578,
      "learning_rate": 4.9204789948364485e-05,
      "loss": 1.4811,
      "step": 278
    },
    {
      "epoch": 0.5224719101123596,
      "grad_norm": 0.5688268542289734,
      "learning_rate": 4.919109506944106e-05,
      "loss": 1.6387,
      "step": 279
    },
    {
      "epoch": 0.5243445692883895,
      "grad_norm": 0.4734685719013214,
      "learning_rate": 4.917728520923738e-05,
      "loss": 1.4663,
      "step": 280
    },
    {
      "epoch": 0.5262172284644194,
      "grad_norm": 0.5545878410339355,
      "learning_rate": 4.916336043339228e-05,
      "loss": 1.6388,
      "step": 281
    },
    {
      "epoch": 0.5280898876404494,
      "grad_norm": 0.4968210756778717,
      "learning_rate": 4.9149320808090826e-05,
      "loss": 1.5215,
      "step": 282
    },
    {
      "epoch": 0.5299625468164794,
      "grad_norm": 0.5002339482307434,
      "learning_rate": 4.913516640006392e-05,
      "loss": 1.5194,
      "step": 283
    },
    {
      "epoch": 0.5318352059925093,
      "grad_norm": 0.524409830570221,
      "learning_rate": 4.912089727658804e-05,
      "loss": 1.6841,
      "step": 284
    },
    {
      "epoch": 0.5337078651685393,
      "grad_norm": 0.5092354416847229,
      "learning_rate": 4.910651350548494e-05,
      "loss": 1.6305,
      "step": 285
    },
    {
      "epoch": 0.5355805243445693,
      "grad_norm": 0.4913199841976166,
      "learning_rate": 4.909201515512128e-05,
      "loss": 1.5249,
      "step": 286
    },
    {
      "epoch": 0.5374531835205992,
      "grad_norm": 0.4958663284778595,
      "learning_rate": 4.907740229440832e-05,
      "loss": 1.4308,
      "step": 287
    },
    {
      "epoch": 0.5393258426966292,
      "grad_norm": 0.5400213599205017,
      "learning_rate": 4.9062674992801594e-05,
      "loss": 1.5988,
      "step": 288
    },
    {
      "epoch": 0.5411985018726592,
      "grad_norm": 0.4825943112373352,
      "learning_rate": 4.904783332030058e-05,
      "loss": 1.5968,
      "step": 289
    },
    {
      "epoch": 0.5430711610486891,
      "grad_norm": 0.5024287104606628,
      "learning_rate": 4.903287734744836e-05,
      "loss": 1.6109,
      "step": 290
    },
    {
      "epoch": 0.5449438202247191,
      "grad_norm": 0.5233958959579468,
      "learning_rate": 4.90178071453313e-05,
      "loss": 1.6321,
      "step": 291
    },
    {
      "epoch": 0.5468164794007491,
      "grad_norm": 0.4796085059642792,
      "learning_rate": 4.900262278557869e-05,
      "loss": 1.6146,
      "step": 292
    },
    {
      "epoch": 0.548689138576779,
      "grad_norm": 0.5503155589103699,
      "learning_rate": 4.898732434036244e-05,
      "loss": 1.6705,
      "step": 293
    },
    {
      "epoch": 0.550561797752809,
      "grad_norm": 0.49318838119506836,
      "learning_rate": 4.897191188239667e-05,
      "loss": 1.6597,
      "step": 294
    },
    {
      "epoch": 0.552434456928839,
      "grad_norm": 0.5290516018867493,
      "learning_rate": 4.895638548493745e-05,
      "loss": 1.5039,
      "step": 295
    },
    {
      "epoch": 0.5543071161048689,
      "grad_norm": 0.5284315347671509,
      "learning_rate": 4.894074522178239e-05,
      "loss": 1.4915,
      "step": 296
    },
    {
      "epoch": 0.5561797752808989,
      "grad_norm": 0.511962890625,
      "learning_rate": 4.892499116727032e-05,
      "loss": 1.4925,
      "step": 297
    },
    {
      "epoch": 0.5580524344569289,
      "grad_norm": 0.49187493324279785,
      "learning_rate": 4.8909123396280894e-05,
      "loss": 1.5265,
      "step": 298
    },
    {
      "epoch": 0.5599250936329588,
      "grad_norm": 0.48019376397132874,
      "learning_rate": 4.88931419842343e-05,
      "loss": 1.471,
      "step": 299
    },
    {
      "epoch": 0.5617977528089888,
      "grad_norm": 0.5110809206962585,
      "learning_rate": 4.8877047007090847e-05,
      "loss": 1.5521,
      "step": 300
    },
    {
      "epoch": 0.5636704119850188,
      "grad_norm": 0.5711036920547485,
      "learning_rate": 4.8860838541350644e-05,
      "loss": 1.6883,
      "step": 301
    },
    {
      "epoch": 0.5655430711610487,
      "grad_norm": 0.5055801272392273,
      "learning_rate": 4.88445166640532e-05,
      "loss": 1.5051,
      "step": 302
    },
    {
      "epoch": 0.5674157303370787,
      "grad_norm": 0.4945031702518463,
      "learning_rate": 4.882808145277705e-05,
      "loss": 1.4923,
      "step": 303
    },
    {
      "epoch": 0.5692883895131086,
      "grad_norm": 0.5012146234512329,
      "learning_rate": 4.881153298563947e-05,
      "loss": 1.5702,
      "step": 304
    },
    {
      "epoch": 0.5711610486891385,
      "grad_norm": 0.5835188627243042,
      "learning_rate": 4.8794871341296e-05,
      "loss": 1.5708,
      "step": 305
    },
    {
      "epoch": 0.5730337078651685,
      "grad_norm": 0.5153496861457825,
      "learning_rate": 4.877809659894012e-05,
      "loss": 1.5661,
      "step": 306
    },
    {
      "epoch": 0.5749063670411985,
      "grad_norm": 0.5514380931854248,
      "learning_rate": 4.876120883830288e-05,
      "loss": 1.5321,
      "step": 307
    },
    {
      "epoch": 0.5767790262172284,
      "grad_norm": 0.5489006042480469,
      "learning_rate": 4.8744208139652526e-05,
      "loss": 1.6192,
      "step": 308
    },
    {
      "epoch": 0.5786516853932584,
      "grad_norm": 0.5330954194068909,
      "learning_rate": 4.872709458379407e-05,
      "loss": 1.5409,
      "step": 309
    },
    {
      "epoch": 0.5805243445692884,
      "grad_norm": 0.5306369662284851,
      "learning_rate": 4.8709868252068947e-05,
      "loss": 1.5681,
      "step": 310
    },
    {
      "epoch": 0.5823970037453183,
      "grad_norm": 0.5339818000793457,
      "learning_rate": 4.8692529226354635e-05,
      "loss": 1.5991,
      "step": 311
    },
    {
      "epoch": 0.5842696629213483,
      "grad_norm": 0.4862052798271179,
      "learning_rate": 4.8675077589064247e-05,
      "loss": 1.5329,
      "step": 312
    },
    {
      "epoch": 0.5861423220973783,
      "grad_norm": 0.5422860383987427,
      "learning_rate": 4.865751342314614e-05,
      "loss": 1.4008,
      "step": 313
    },
    {
      "epoch": 0.5880149812734082,
      "grad_norm": 0.47874289751052856,
      "learning_rate": 4.863983681208352e-05,
      "loss": 1.4877,
      "step": 314
    },
    {
      "epoch": 0.5898876404494382,
      "grad_norm": 0.5699431300163269,
      "learning_rate": 4.862204783989406e-05,
      "loss": 1.5641,
      "step": 315
    },
    {
      "epoch": 0.5917602996254682,
      "grad_norm": 0.5244682431221008,
      "learning_rate": 4.8604146591129485e-05,
      "loss": 1.495,
      "step": 316
    },
    {
      "epoch": 0.5936329588014981,
      "grad_norm": 0.5290126800537109,
      "learning_rate": 4.8586133150875165e-05,
      "loss": 1.5505,
      "step": 317
    },
    {
      "epoch": 0.5955056179775281,
      "grad_norm": 0.54109787940979,
      "learning_rate": 4.856800760474973e-05,
      "loss": 1.4514,
      "step": 318
    },
    {
      "epoch": 0.5973782771535581,
      "grad_norm": 0.5529168844223022,
      "learning_rate": 4.8549770038904676e-05,
      "loss": 1.5436,
      "step": 319
    },
    {
      "epoch": 0.599250936329588,
      "grad_norm": 0.5439868569374084,
      "learning_rate": 4.853142054002388e-05,
      "loss": 1.5953,
      "step": 320
    },
    {
      "epoch": 0.601123595505618,
      "grad_norm": 0.5372373461723328,
      "learning_rate": 4.851295919532329e-05,
      "loss": 1.5565,
      "step": 321
    },
    {
      "epoch": 0.602996254681648,
      "grad_norm": 0.5294420719146729,
      "learning_rate": 4.849438609255045e-05,
      "loss": 1.4904,
      "step": 322
    },
    {
      "epoch": 0.6048689138576779,
      "grad_norm": 0.5943358540534973,
      "learning_rate": 4.847570131998408e-05,
      "loss": 1.5743,
      "step": 323
    },
    {
      "epoch": 0.6067415730337079,
      "grad_norm": 0.5680549144744873,
      "learning_rate": 4.845690496643368e-05,
      "loss": 1.6196,
      "step": 324
    },
    {
      "epoch": 0.6086142322097379,
      "grad_norm": 0.5187797546386719,
      "learning_rate": 4.8437997121239097e-05,
      "loss": 1.3533,
      "step": 325
    },
    {
      "epoch": 0.6104868913857678,
      "grad_norm": 0.49138620495796204,
      "learning_rate": 4.8418977874270114e-05,
      "loss": 1.4499,
      "step": 326
    },
    {
      "epoch": 0.6123595505617978,
      "grad_norm": 0.5311278700828552,
      "learning_rate": 4.8399847315926e-05,
      "loss": 1.5782,
      "step": 327
    },
    {
      "epoch": 0.6142322097378277,
      "grad_norm": 0.56284499168396,
      "learning_rate": 4.838060553713509e-05,
      "loss": 1.4447,
      "step": 328
    },
    {
      "epoch": 0.6161048689138576,
      "grad_norm": 0.5557159185409546,
      "learning_rate": 4.836125262935436e-05,
      "loss": 1.4676,
      "step": 329
    },
    {
      "epoch": 0.6179775280898876,
      "grad_norm": 0.5069503784179688,
      "learning_rate": 4.834178868456899e-05,
      "loss": 1.5043,
      "step": 330
    },
    {
      "epoch": 0.6198501872659176,
      "grad_norm": 0.5256422758102417,
      "learning_rate": 4.832221379529191e-05,
      "loss": 1.5073,
      "step": 331
    },
    {
      "epoch": 0.6217228464419475,
      "grad_norm": 0.5083180069923401,
      "learning_rate": 4.830252805456339e-05,
      "loss": 1.4534,
      "step": 332
    },
    {
      "epoch": 0.6235955056179775,
      "grad_norm": 0.558099091053009,
      "learning_rate": 4.8282731555950565e-05,
      "loss": 1.5982,
      "step": 333
    },
    {
      "epoch": 0.6254681647940075,
      "grad_norm": 0.5412806868553162,
      "learning_rate": 4.8262824393547025e-05,
      "loss": 1.5415,
      "step": 334
    },
    {
      "epoch": 0.6273408239700374,
      "grad_norm": 0.6283893585205078,
      "learning_rate": 4.824280666197233e-05,
      "loss": 1.6048,
      "step": 335
    },
    {
      "epoch": 0.6292134831460674,
      "grad_norm": 0.5493637919425964,
      "learning_rate": 4.82226784563716e-05,
      "loss": 1.5708,
      "step": 336
    },
    {
      "epoch": 0.6310861423220974,
      "grad_norm": 0.5414080023765564,
      "learning_rate": 4.8202439872415026e-05,
      "loss": 1.5841,
      "step": 337
    },
    {
      "epoch": 0.6329588014981273,
      "grad_norm": 0.5619045495986938,
      "learning_rate": 4.818209100629745e-05,
      "loss": 1.6123,
      "step": 338
    },
    {
      "epoch": 0.6348314606741573,
      "grad_norm": 0.5456125140190125,
      "learning_rate": 4.8161631954737863e-05,
      "loss": 1.5308,
      "step": 339
    },
    {
      "epoch": 0.6367041198501873,
      "grad_norm": 0.5318711996078491,
      "learning_rate": 4.814106281497899e-05,
      "loss": 1.5125,
      "step": 340
    },
    {
      "epoch": 0.6385767790262172,
      "grad_norm": 0.5692934393882751,
      "learning_rate": 4.8120383684786816e-05,
      "loss": 1.5308,
      "step": 341
    },
    {
      "epoch": 0.6404494382022472,
      "grad_norm": 0.6086012721061707,
      "learning_rate": 4.809959466245011e-05,
      "loss": 1.4558,
      "step": 342
    },
    {
      "epoch": 0.6423220973782772,
      "grad_norm": 0.5348381996154785,
      "learning_rate": 4.807869584677994e-05,
      "loss": 1.4218,
      "step": 343
    },
    {
      "epoch": 0.6441947565543071,
      "grad_norm": 0.5694422125816345,
      "learning_rate": 4.805768733710926e-05,
      "loss": 1.6167,
      "step": 344
    },
    {
      "epoch": 0.6460674157303371,
      "grad_norm": 0.5543438196182251,
      "learning_rate": 4.8036569233292385e-05,
      "loss": 1.6424,
      "step": 345
    },
    {
      "epoch": 0.6479400749063671,
      "grad_norm": 0.48457780480384827,
      "learning_rate": 4.8015341635704535e-05,
      "loss": 1.4072,
      "step": 346
    },
    {
      "epoch": 0.649812734082397,
      "grad_norm": 0.5673834085464478,
      "learning_rate": 4.7994004645241374e-05,
      "loss": 1.6001,
      "step": 347
    },
    {
      "epoch": 0.651685393258427,
      "grad_norm": 0.5700641870498657,
      "learning_rate": 4.79725583633185e-05,
      "loss": 1.5595,
      "step": 348
    },
    {
      "epoch": 0.653558052434457,
      "grad_norm": 0.5455413460731506,
      "learning_rate": 4.795100289187099e-05,
      "loss": 1.4249,
      "step": 349
    },
    {
      "epoch": 0.6554307116104869,
      "grad_norm": 0.5112595558166504,
      "learning_rate": 4.792933833335287e-05,
      "loss": 1.5295,
      "step": 350
    },
    {
      "epoch": 0.6573033707865169,
      "grad_norm": 0.5308012366294861,
      "learning_rate": 4.790756479073672e-05,
      "loss": 1.5187,
      "step": 351
    },
    {
      "epoch": 0.6591760299625468,
      "grad_norm": 0.5509033203125,
      "learning_rate": 4.788568236751307e-05,
      "loss": 1.4717,
      "step": 352
    },
    {
      "epoch": 0.6610486891385767,
      "grad_norm": 0.5935831069946289,
      "learning_rate": 4.786369116769e-05,
      "loss": 1.5537,
      "step": 353
    },
    {
      "epoch": 0.6629213483146067,
      "grad_norm": 0.5081677436828613,
      "learning_rate": 4.784159129579259e-05,
      "loss": 1.5478,
      "step": 354
    },
    {
      "epoch": 0.6647940074906367,
      "grad_norm": 0.5404625535011292,
      "learning_rate": 4.781938285686245e-05,
      "loss": 1.4366,
      "step": 355
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.6922754049301147,
      "learning_rate": 4.779706595645721e-05,
      "loss": 1.3929,
      "step": 356
    },
    {
      "epoch": 0.6685393258426966,
      "grad_norm": 0.5203282237052917,
      "learning_rate": 4.777464070065004e-05,
      "loss": 1.5464,
      "step": 357
    },
    {
      "epoch": 0.6704119850187266,
      "grad_norm": 0.5389352440834045,
      "learning_rate": 4.775210719602909e-05,
      "loss": 1.6603,
      "step": 358
    },
    {
      "epoch": 0.6722846441947565,
      "grad_norm": 0.5584748387336731,
      "learning_rate": 4.772946554969706e-05,
      "loss": 1.6016,
      "step": 359
    },
    {
      "epoch": 0.6741573033707865,
      "grad_norm": 0.5195223093032837,
      "learning_rate": 4.7706715869270635e-05,
      "loss": 1.5615,
      "step": 360
    },
    {
      "epoch": 0.6760299625468165,
      "grad_norm": 0.5426691174507141,
      "learning_rate": 4.768385826287999e-05,
      "loss": 1.5703,
      "step": 361
    },
    {
      "epoch": 0.6779026217228464,
      "grad_norm": 0.6265625953674316,
      "learning_rate": 4.766089283916828e-05,
      "loss": 1.417,
      "step": 362
    },
    {
      "epoch": 0.6797752808988764,
      "grad_norm": 0.5852693915367126,
      "learning_rate": 4.763781970729111e-05,
      "loss": 1.6178,
      "step": 363
    },
    {
      "epoch": 0.6816479400749064,
      "grad_norm": 0.5303862690925598,
      "learning_rate": 4.761463897691604e-05,
      "loss": 1.4927,
      "step": 364
    },
    {
      "epoch": 0.6835205992509363,
      "grad_norm": 0.5880848169326782,
      "learning_rate": 4.759135075822204e-05,
      "loss": 1.582,
      "step": 365
    },
    {
      "epoch": 0.6853932584269663,
      "grad_norm": 0.5827677845954895,
      "learning_rate": 4.756795516189899e-05,
      "loss": 1.6638,
      "step": 366
    },
    {
      "epoch": 0.6872659176029963,
      "grad_norm": 0.5496044158935547,
      "learning_rate": 4.7544452299147135e-05,
      "loss": 1.5652,
      "step": 367
    },
    {
      "epoch": 0.6891385767790262,
      "grad_norm": 0.560042679309845,
      "learning_rate": 4.752084228167654e-05,
      "loss": 1.6401,
      "step": 368
    },
    {
      "epoch": 0.6910112359550562,
      "grad_norm": 0.6079166531562805,
      "learning_rate": 4.7497125221706616e-05,
      "loss": 1.4769,
      "step": 369
    },
    {
      "epoch": 0.6928838951310862,
      "grad_norm": 0.5318018198013306,
      "learning_rate": 4.747330123196553e-05,
      "loss": 1.5424,
      "step": 370
    },
    {
      "epoch": 0.6947565543071161,
      "grad_norm": 0.5000928640365601,
      "learning_rate": 4.74493704256897e-05,
      "loss": 1.4197,
      "step": 371
    },
    {
      "epoch": 0.6966292134831461,
      "grad_norm": 0.6245346069335938,
      "learning_rate": 4.7425332916623225e-05,
      "loss": 1.3791,
      "step": 372
    },
    {
      "epoch": 0.6985018726591761,
      "grad_norm": 0.5771006345748901,
      "learning_rate": 4.7401188819017404e-05,
      "loss": 1.4901,
      "step": 373
    },
    {
      "epoch": 0.700374531835206,
      "grad_norm": 0.5903903841972351,
      "learning_rate": 4.737693824763013e-05,
      "loss": 1.6937,
      "step": 374
    },
    {
      "epoch": 0.702247191011236,
      "grad_norm": 0.6461037397384644,
      "learning_rate": 4.735258131772538e-05,
      "loss": 1.4697,
      "step": 375
    },
    {
      "epoch": 0.704119850187266,
      "grad_norm": 0.5642143487930298,
      "learning_rate": 4.732811814507264e-05,
      "loss": 1.3845,
      "step": 376
    },
    {
      "epoch": 0.7059925093632958,
      "grad_norm": 0.5590445399284363,
      "learning_rate": 4.73035488459464e-05,
      "loss": 1.5049,
      "step": 377
    },
    {
      "epoch": 0.7078651685393258,
      "grad_norm": 0.5944081544876099,
      "learning_rate": 4.727887353712556e-05,
      "loss": 1.4824,
      "step": 378
    },
    {
      "epoch": 0.7097378277153558,
      "grad_norm": 0.6306272745132446,
      "learning_rate": 4.725409233589288e-05,
      "loss": 1.4873,
      "step": 379
    },
    {
      "epoch": 0.7116104868913857,
      "grad_norm": 0.5969378352165222,
      "learning_rate": 4.722920536003444e-05,
      "loss": 1.596,
      "step": 380
    },
    {
      "epoch": 0.7134831460674157,
      "grad_norm": 0.6019903421401978,
      "learning_rate": 4.7204212727839085e-05,
      "loss": 1.4348,
      "step": 381
    },
    {
      "epoch": 0.7153558052434457,
      "grad_norm": 0.5307039618492126,
      "learning_rate": 4.717911455809782e-05,
      "loss": 1.4961,
      "step": 382
    },
    {
      "epoch": 0.7172284644194756,
      "grad_norm": 0.6643754839897156,
      "learning_rate": 4.7153910970103294e-05,
      "loss": 1.3842,
      "step": 383
    },
    {
      "epoch": 0.7191011235955056,
      "grad_norm": 0.6264708638191223,
      "learning_rate": 4.7128602083649206e-05,
      "loss": 1.5461,
      "step": 384
    },
    {
      "epoch": 0.7209737827715356,
      "grad_norm": 0.5551989078521729,
      "learning_rate": 4.710318801902974e-05,
      "loss": 1.5888,
      "step": 385
    },
    {
      "epoch": 0.7228464419475655,
      "grad_norm": 0.5758841633796692,
      "learning_rate": 4.707766889703902e-05,
      "loss": 1.286,
      "step": 386
    },
    {
      "epoch": 0.7247191011235955,
      "grad_norm": 0.6940587759017944,
      "learning_rate": 4.705204483897048e-05,
      "loss": 1.4614,
      "step": 387
    },
    {
      "epoch": 0.7265917602996255,
      "grad_norm": 0.6325831413269043,
      "learning_rate": 4.702631596661632e-05,
      "loss": 1.5538,
      "step": 388
    },
    {
      "epoch": 0.7284644194756554,
      "grad_norm": 0.6713495850563049,
      "learning_rate": 4.700048240226697e-05,
      "loss": 1.6068,
      "step": 389
    },
    {
      "epoch": 0.7303370786516854,
      "grad_norm": 0.5395624041557312,
      "learning_rate": 4.697454426871041e-05,
      "loss": 1.5282,
      "step": 390
    },
    {
      "epoch": 0.7322097378277154,
      "grad_norm": 0.5392687916755676,
      "learning_rate": 4.6948501689231674e-05,
      "loss": 1.454,
      "step": 391
    },
    {
      "epoch": 0.7340823970037453,
      "grad_norm": 0.597217321395874,
      "learning_rate": 4.6922354787612235e-05,
      "loss": 1.4519,
      "step": 392
    },
    {
      "epoch": 0.7359550561797753,
      "grad_norm": 0.5508166551589966,
      "learning_rate": 4.6896103688129385e-05,
      "loss": 1.3885,
      "step": 393
    },
    {
      "epoch": 0.7378277153558053,
      "grad_norm": 0.5807058215141296,
      "learning_rate": 4.68697485155557e-05,
      "loss": 1.6481,
      "step": 394
    },
    {
      "epoch": 0.7397003745318352,
      "grad_norm": 0.5929625034332275,
      "learning_rate": 4.6843289395158416e-05,
      "loss": 1.5355,
      "step": 395
    },
    {
      "epoch": 0.7415730337078652,
      "grad_norm": 0.6043530106544495,
      "learning_rate": 4.6816726452698825e-05,
      "loss": 1.7395,
      "step": 396
    },
    {
      "epoch": 0.7434456928838952,
      "grad_norm": 0.5456154942512512,
      "learning_rate": 4.679005981443169e-05,
      "loss": 1.4841,
      "step": 397
    },
    {
      "epoch": 0.7453183520599251,
      "grad_norm": 0.5798966288566589,
      "learning_rate": 4.676328960710467e-05,
      "loss": 1.4766,
      "step": 398
    },
    {
      "epoch": 0.7471910112359551,
      "grad_norm": 0.6088434457778931,
      "learning_rate": 4.673641595795766e-05,
      "loss": 1.4869,
      "step": 399
    },
    {
      "epoch": 0.7490636704119851,
      "grad_norm": 0.5987531542778015,
      "learning_rate": 4.670943899472222e-05,
      "loss": 1.4857,
      "step": 400
    },
    {
      "epoch": 0.7509363295880149,
      "grad_norm": 0.5846810340881348,
      "learning_rate": 4.6682358845621e-05,
      "loss": 1.474,
      "step": 401
    },
    {
      "epoch": 0.7528089887640449,
      "grad_norm": 0.6350507736206055,
      "learning_rate": 4.6655175639367064e-05,
      "loss": 1.6041,
      "step": 402
    },
    {
      "epoch": 0.7546816479400749,
      "grad_norm": 0.5428797006607056,
      "learning_rate": 4.6627889505163326e-05,
      "loss": 1.5503,
      "step": 403
    },
    {
      "epoch": 0.7565543071161048,
      "grad_norm": 0.6063888669013977,
      "learning_rate": 4.660050057270191e-05,
      "loss": 1.4946,
      "step": 404
    },
    {
      "epoch": 0.7584269662921348,
      "grad_norm": 0.5902723073959351,
      "learning_rate": 4.657300897216355e-05,
      "loss": 1.4759,
      "step": 405
    },
    {
      "epoch": 0.7602996254681648,
      "grad_norm": 0.6658123731613159,
      "learning_rate": 4.6545414834216974e-05,
      "loss": 1.4666,
      "step": 406
    },
    {
      "epoch": 0.7621722846441947,
      "grad_norm": 0.6153548359870911,
      "learning_rate": 4.6517718290018246e-05,
      "loss": 1.5125,
      "step": 407
    },
    {
      "epoch": 0.7640449438202247,
      "grad_norm": 0.6019141674041748,
      "learning_rate": 4.648991947121022e-05,
      "loss": 1.5322,
      "step": 408
    },
    {
      "epoch": 0.7659176029962547,
      "grad_norm": 0.5533401370048523,
      "learning_rate": 4.646201850992181e-05,
      "loss": 1.5519,
      "step": 409
    },
    {
      "epoch": 0.7677902621722846,
      "grad_norm": 0.5879760384559631,
      "learning_rate": 4.643401553876747e-05,
      "loss": 1.4765,
      "step": 410
    },
    {
      "epoch": 0.7696629213483146,
      "grad_norm": 0.5855856537818909,
      "learning_rate": 4.6405910690846465e-05,
      "loss": 1.5427,
      "step": 411
    },
    {
      "epoch": 0.7715355805243446,
      "grad_norm": 0.6749649047851562,
      "learning_rate": 4.6377704099742316e-05,
      "loss": 1.4965,
      "step": 412
    },
    {
      "epoch": 0.7734082397003745,
      "grad_norm": 0.5689771771430969,
      "learning_rate": 4.634939589952212e-05,
      "loss": 1.4174,
      "step": 413
    },
    {
      "epoch": 0.7752808988764045,
      "grad_norm": 0.6006414294242859,
      "learning_rate": 4.632098622473593e-05,
      "loss": 1.4976,
      "step": 414
    },
    {
      "epoch": 0.7771535580524345,
      "grad_norm": 0.6302584409713745,
      "learning_rate": 4.6292475210416106e-05,
      "loss": 1.4713,
      "step": 415
    },
    {
      "epoch": 0.7790262172284644,
      "grad_norm": 0.5357658267021179,
      "learning_rate": 4.62638629920767e-05,
      "loss": 1.615,
      "step": 416
    },
    {
      "epoch": 0.7808988764044944,
      "grad_norm": 0.602067768573761,
      "learning_rate": 4.623514970571275e-05,
      "loss": 1.562,
      "step": 417
    },
    {
      "epoch": 0.7827715355805244,
      "grad_norm": 0.6606037616729736,
      "learning_rate": 4.620633548779972e-05,
      "loss": 1.4152,
      "step": 418
    },
    {
      "epoch": 0.7846441947565543,
      "grad_norm": 0.6046567559242249,
      "learning_rate": 4.6177420475292776e-05,
      "loss": 1.4492,
      "step": 419
    },
    {
      "epoch": 0.7865168539325843,
      "grad_norm": 0.6398761868476868,
      "learning_rate": 4.614840480562618e-05,
      "loss": 1.502,
      "step": 420
    },
    {
      "epoch": 0.7883895131086143,
      "grad_norm": 0.6229274868965149,
      "learning_rate": 4.611928861671261e-05,
      "loss": 1.4835,
      "step": 421
    },
    {
      "epoch": 0.7902621722846442,
      "grad_norm": 0.5870708227157593,
      "learning_rate": 4.609007204694252e-05,
      "loss": 1.5098,
      "step": 422
    },
    {
      "epoch": 0.7921348314606742,
      "grad_norm": 0.5467736124992371,
      "learning_rate": 4.606075523518348e-05,
      "loss": 1.2835,
      "step": 423
    },
    {
      "epoch": 0.7940074906367042,
      "grad_norm": 0.6249762773513794,
      "learning_rate": 4.6031338320779534e-05,
      "loss": 1.5746,
      "step": 424
    },
    {
      "epoch": 0.795880149812734,
      "grad_norm": 0.6202216744422913,
      "learning_rate": 4.600182144355048e-05,
      "loss": 1.6103,
      "step": 425
    },
    {
      "epoch": 0.797752808988764,
      "grad_norm": 0.5956045985221863,
      "learning_rate": 4.597220474379125e-05,
      "loss": 1.5009,
      "step": 426
    },
    {
      "epoch": 0.799625468164794,
      "grad_norm": 0.70387864112854,
      "learning_rate": 4.594248836227128e-05,
      "loss": 1.5208,
      "step": 427
    },
    {
      "epoch": 0.8014981273408239,
      "grad_norm": 0.6028091907501221,
      "learning_rate": 4.591267244023375e-05,
      "loss": 1.4739,
      "step": 428
    },
    {
      "epoch": 0.8033707865168539,
      "grad_norm": 0.558171272277832,
      "learning_rate": 4.588275711939497e-05,
      "loss": 1.54,
      "step": 429
    },
    {
      "epoch": 0.8052434456928839,
      "grad_norm": 0.5952693819999695,
      "learning_rate": 4.585274254194372e-05,
      "loss": 1.4732,
      "step": 430
    },
    {
      "epoch": 0.8071161048689138,
      "grad_norm": 0.6554426550865173,
      "learning_rate": 4.582262885054052e-05,
      "loss": 1.5009,
      "step": 431
    },
    {
      "epoch": 0.8089887640449438,
      "grad_norm": 0.6595553755760193,
      "learning_rate": 4.5792416188317e-05,
      "loss": 1.4326,
      "step": 432
    },
    {
      "epoch": 0.8108614232209738,
      "grad_norm": 0.6131614446640015,
      "learning_rate": 4.57621046988752e-05,
      "loss": 1.446,
      "step": 433
    },
    {
      "epoch": 0.8127340823970037,
      "grad_norm": 0.612413227558136,
      "learning_rate": 4.573169452628689e-05,
      "loss": 1.4366,
      "step": 434
    },
    {
      "epoch": 0.8146067415730337,
      "grad_norm": 0.5477418899536133,
      "learning_rate": 4.5701185815092894e-05,
      "loss": 1.4949,
      "step": 435
    },
    {
      "epoch": 0.8164794007490637,
      "grad_norm": 0.6203544735908508,
      "learning_rate": 4.567057871030237e-05,
      "loss": 1.4984,
      "step": 436
    },
    {
      "epoch": 0.8183520599250936,
      "grad_norm": 0.6339380145072937,
      "learning_rate": 4.563987335739216e-05,
      "loss": 1.5521,
      "step": 437
    },
    {
      "epoch": 0.8202247191011236,
      "grad_norm": 0.6632719039916992,
      "learning_rate": 4.560906990230609e-05,
      "loss": 1.5814,
      "step": 438
    },
    {
      "epoch": 0.8220973782771536,
      "grad_norm": 0.6095225811004639,
      "learning_rate": 4.557816849145425e-05,
      "loss": 1.4657,
      "step": 439
    },
    {
      "epoch": 0.8239700374531835,
      "grad_norm": 0.6027294397354126,
      "learning_rate": 4.5547169271712345e-05,
      "loss": 1.4277,
      "step": 440
    },
    {
      "epoch": 0.8258426966292135,
      "grad_norm": 0.6215336322784424,
      "learning_rate": 4.551607239042095e-05,
      "loss": 1.382,
      "step": 441
    },
    {
      "epoch": 0.8277153558052435,
      "grad_norm": 0.5859622359275818,
      "learning_rate": 4.5484877995384824e-05,
      "loss": 1.4685,
      "step": 442
    },
    {
      "epoch": 0.8295880149812734,
      "grad_norm": 0.5421558022499084,
      "learning_rate": 4.545358623487224e-05,
      "loss": 1.5079,
      "step": 443
    },
    {
      "epoch": 0.8314606741573034,
      "grad_norm": 0.6584895849227905,
      "learning_rate": 4.542219725761422e-05,
      "loss": 1.447,
      "step": 444
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.5986391305923462,
      "learning_rate": 4.539071121280389e-05,
      "loss": 1.5709,
      "step": 445
    },
    {
      "epoch": 0.8352059925093633,
      "grad_norm": 0.594515860080719,
      "learning_rate": 4.535912825009573e-05,
      "loss": 1.584,
      "step": 446
    },
    {
      "epoch": 0.8370786516853933,
      "grad_norm": 0.6241726279258728,
      "learning_rate": 4.5327448519604854e-05,
      "loss": 1.6581,
      "step": 447
    },
    {
      "epoch": 0.8389513108614233,
      "grad_norm": 0.6634968519210815,
      "learning_rate": 4.5295672171906364e-05,
      "loss": 1.4555,
      "step": 448
    },
    {
      "epoch": 0.8408239700374532,
      "grad_norm": 0.6423235535621643,
      "learning_rate": 4.526379935803455e-05,
      "loss": 1.6214,
      "step": 449
    },
    {
      "epoch": 0.8426966292134831,
      "grad_norm": 0.6426616311073303,
      "learning_rate": 4.5231830229482216e-05,
      "loss": 1.4565,
      "step": 450
    },
    {
      "epoch": 0.8445692883895131,
      "grad_norm": 0.6667292714118958,
      "learning_rate": 4.519976493819996e-05,
      "loss": 1.5757,
      "step": 451
    },
    {
      "epoch": 0.846441947565543,
      "grad_norm": 0.5984494686126709,
      "learning_rate": 4.516760363659546e-05,
      "loss": 1.4159,
      "step": 452
    },
    {
      "epoch": 0.848314606741573,
      "grad_norm": 0.6222307682037354,
      "learning_rate": 4.5135346477532694e-05,
      "loss": 1.4821,
      "step": 453
    },
    {
      "epoch": 0.850187265917603,
      "grad_norm": 0.6679445505142212,
      "learning_rate": 4.510299361433129e-05,
      "loss": 1.6567,
      "step": 454
    },
    {
      "epoch": 0.8520599250936329,
      "grad_norm": 0.5926706790924072,
      "learning_rate": 4.507054520076576e-05,
      "loss": 1.4827,
      "step": 455
    },
    {
      "epoch": 0.8539325842696629,
      "grad_norm": 0.6887377500534058,
      "learning_rate": 4.503800139106475e-05,
      "loss": 1.4809,
      "step": 456
    },
    {
      "epoch": 0.8558052434456929,
      "grad_norm": 0.6185072064399719,
      "learning_rate": 4.500536233991036e-05,
      "loss": 1.4913,
      "step": 457
    },
    {
      "epoch": 0.8576779026217228,
      "grad_norm": 0.5711035132408142,
      "learning_rate": 4.497262820243733e-05,
      "loss": 1.4673,
      "step": 458
    },
    {
      "epoch": 0.8595505617977528,
      "grad_norm": 0.6949212551116943,
      "learning_rate": 4.49397991342324e-05,
      "loss": 1.5749,
      "step": 459
    },
    {
      "epoch": 0.8614232209737828,
      "grad_norm": 0.6377492547035217,
      "learning_rate": 4.490687529133347e-05,
      "loss": 1.4228,
      "step": 460
    },
    {
      "epoch": 0.8632958801498127,
      "grad_norm": 0.6196776628494263,
      "learning_rate": 4.4873856830228956e-05,
      "loss": 1.523,
      "step": 461
    },
    {
      "epoch": 0.8651685393258427,
      "grad_norm": 0.6355713605880737,
      "learning_rate": 4.4840743907856964e-05,
      "loss": 1.5543,
      "step": 462
    },
    {
      "epoch": 0.8670411985018727,
      "grad_norm": 0.6542779803276062,
      "learning_rate": 4.48075366816046e-05,
      "loss": 1.4585,
      "step": 463
    },
    {
      "epoch": 0.8689138576779026,
      "grad_norm": 0.6580648422241211,
      "learning_rate": 4.477423530930718e-05,
      "loss": 1.4987,
      "step": 464
    },
    {
      "epoch": 0.8707865168539326,
      "grad_norm": 0.7139855623245239,
      "learning_rate": 4.474083994924751e-05,
      "loss": 1.7015,
      "step": 465
    },
    {
      "epoch": 0.8726591760299626,
      "grad_norm": 0.6516953110694885,
      "learning_rate": 4.470735076015513e-05,
      "loss": 1.4435,
      "step": 466
    },
    {
      "epoch": 0.8745318352059925,
      "grad_norm": 0.5935096740722656,
      "learning_rate": 4.467376790120555e-05,
      "loss": 1.5993,
      "step": 467
    },
    {
      "epoch": 0.8764044943820225,
      "grad_norm": 0.6349415183067322,
      "learning_rate": 4.464009153201949e-05,
      "loss": 1.5635,
      "step": 468
    },
    {
      "epoch": 0.8782771535580525,
      "grad_norm": 0.7298821806907654,
      "learning_rate": 4.460632181266213e-05,
      "loss": 1.6208,
      "step": 469
    },
    {
      "epoch": 0.8801498127340824,
      "grad_norm": 0.6330360174179077,
      "learning_rate": 4.4572458903642354e-05,
      "loss": 1.4578,
      "step": 470
    },
    {
      "epoch": 0.8820224719101124,
      "grad_norm": 0.7239656448364258,
      "learning_rate": 4.4538502965911974e-05,
      "loss": 1.4654,
      "step": 471
    },
    {
      "epoch": 0.8838951310861424,
      "grad_norm": 0.6726431846618652,
      "learning_rate": 4.450445416086498e-05,
      "loss": 1.6424,
      "step": 472
    },
    {
      "epoch": 0.8857677902621723,
      "grad_norm": 0.6254916191101074,
      "learning_rate": 4.447031265033675e-05,
      "loss": 1.5165,
      "step": 473
    },
    {
      "epoch": 0.8876404494382022,
      "grad_norm": 0.6795141100883484,
      "learning_rate": 4.4436078596603305e-05,
      "loss": 1.4982,
      "step": 474
    },
    {
      "epoch": 0.8895131086142322,
      "grad_norm": 0.5571033358573914,
      "learning_rate": 4.440175216238052e-05,
      "loss": 1.3725,
      "step": 475
    },
    {
      "epoch": 0.8913857677902621,
      "grad_norm": 0.6517658829689026,
      "learning_rate": 4.436733351082336e-05,
      "loss": 1.5846,
      "step": 476
    },
    {
      "epoch": 0.8932584269662921,
      "grad_norm": 0.6886183619499207,
      "learning_rate": 4.433282280552512e-05,
      "loss": 1.5206,
      "step": 477
    },
    {
      "epoch": 0.8951310861423221,
      "grad_norm": 0.6537775993347168,
      "learning_rate": 4.429822021051662e-05,
      "loss": 1.4459,
      "step": 478
    },
    {
      "epoch": 0.897003745318352,
      "grad_norm": 0.6701787114143372,
      "learning_rate": 4.426352589026541e-05,
      "loss": 1.5622,
      "step": 479
    },
    {
      "epoch": 0.898876404494382,
      "grad_norm": 0.6152222156524658,
      "learning_rate": 4.422874000967505e-05,
      "loss": 1.431,
      "step": 480
    },
    {
      "epoch": 0.900749063670412,
      "grad_norm": 0.7083595395088196,
      "learning_rate": 4.419386273408428e-05,
      "loss": 1.4192,
      "step": 481
    },
    {
      "epoch": 0.9026217228464419,
      "grad_norm": 0.6722803115844727,
      "learning_rate": 4.415889422926623e-05,
      "loss": 1.5021,
      "step": 482
    },
    {
      "epoch": 0.9044943820224719,
      "grad_norm": 0.6735994815826416,
      "learning_rate": 4.4123834661427665e-05,
      "loss": 1.572,
      "step": 483
    },
    {
      "epoch": 0.9063670411985019,
      "grad_norm": 0.7045339345932007,
      "learning_rate": 4.408868419720816e-05,
      "loss": 1.5372,
      "step": 484
    },
    {
      "epoch": 0.9082397003745318,
      "grad_norm": 0.6616561412811279,
      "learning_rate": 4.405344300367934e-05,
      "loss": 1.4453,
      "step": 485
    },
    {
      "epoch": 0.9101123595505618,
      "grad_norm": 0.595050573348999,
      "learning_rate": 4.4018111248344065e-05,
      "loss": 1.4015,
      "step": 486
    },
    {
      "epoch": 0.9119850187265918,
      "grad_norm": 0.629509687423706,
      "learning_rate": 4.398268909913562e-05,
      "loss": 1.4259,
      "step": 487
    },
    {
      "epoch": 0.9138576779026217,
      "grad_norm": 0.7117917537689209,
      "learning_rate": 4.394717672441697e-05,
      "loss": 1.5047,
      "step": 488
    },
    {
      "epoch": 0.9157303370786517,
      "grad_norm": 0.6150439977645874,
      "learning_rate": 4.39115742929799e-05,
      "loss": 1.4506,
      "step": 489
    },
    {
      "epoch": 0.9176029962546817,
      "grad_norm": 0.6815507411956787,
      "learning_rate": 4.3875881974044256e-05,
      "loss": 1.4859,
      "step": 490
    },
    {
      "epoch": 0.9194756554307116,
      "grad_norm": 0.6565738916397095,
      "learning_rate": 4.384009993725709e-05,
      "loss": 1.4943,
      "step": 491
    },
    {
      "epoch": 0.9213483146067416,
      "grad_norm": 0.6918242573738098,
      "learning_rate": 4.3804228352691935e-05,
      "loss": 1.5349,
      "step": 492
    },
    {
      "epoch": 0.9232209737827716,
      "grad_norm": 0.6857613921165466,
      "learning_rate": 4.3768267390847906e-05,
      "loss": 1.6027,
      "step": 493
    },
    {
      "epoch": 0.9250936329588015,
      "grad_norm": 0.761750340461731,
      "learning_rate": 4.373221722264896e-05,
      "loss": 1.4706,
      "step": 494
    },
    {
      "epoch": 0.9269662921348315,
      "grad_norm": 0.642249584197998,
      "learning_rate": 4.369607801944304e-05,
      "loss": 1.4745,
      "step": 495
    },
    {
      "epoch": 0.9288389513108615,
      "grad_norm": 0.6283730268478394,
      "learning_rate": 4.365984995300129e-05,
      "loss": 1.5516,
      "step": 496
    },
    {
      "epoch": 0.9307116104868914,
      "grad_norm": 0.6271697878837585,
      "learning_rate": 4.36235331955172e-05,
      "loss": 1.5119,
      "step": 497
    },
    {
      "epoch": 0.9325842696629213,
      "grad_norm": 0.6692715883255005,
      "learning_rate": 4.358712791960583e-05,
      "loss": 1.5328,
      "step": 498
    },
    {
      "epoch": 0.9344569288389513,
      "grad_norm": 0.6210083365440369,
      "learning_rate": 4.355063429830298e-05,
      "loss": 1.6398,
      "step": 499
    },
    {
      "epoch": 0.9363295880149812,
      "grad_norm": 0.653144359588623,
      "learning_rate": 4.351405250506434e-05,
      "loss": 1.546,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1602,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.806085534627021e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
